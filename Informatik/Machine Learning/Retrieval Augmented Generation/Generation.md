**Generation Pipeline**: The set of processes that is employed to search and retrieve information from the knowledge base to generate responses to user queries. It facilitates real-time interaction with users.

**Information Retrieval**: IR is the science of searching. Whether you are searching for information in a document, or searching for document themselves, it falls under the gamut of Information Retrieval.

**Retriever**: The component of the generation pipeline that uses an algorithm to search and retrieve relevant information from the knowledge base.

**Boolean retrieval**: A simple keyword-based search where Boolean logic is used to match documents with queries based on absence or presence of the words. Documents are retrieved if they contain the exact terms in the query, often combined with AND, NOT and OR operators.

**TF-IDF**: Term Frequency-Inverse Document Frequency is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It assigns higher weights to words that appear frequently in a document but infrequently across the corpus.

![None](https://miro.medium.com/v2/resize:fit:700/1*9VLpWevJ2a-Fnf6d-05gkg.png)


**BM25**: Best Match 25 is an advanced probabilistic model used to rank documents based on the query terms appearing in each document. It is part of the family of probabilistic information retrieval models and is considered an advancement over the classic TF-IDF model. The improvement that BM25 brings is that it adjusts for the length of the documents so that longer documents do not unfairly get higher scores.

**Static Word Embeddings**: Static embeddings like Word2Vec and GloVe represent words as dense vectors in a continuous vector space, capturing semantic relationships based on context. For instance, "king" — "man" + "woman" approximates "queen". These embeddings can capture nuances like similarity and analogy, which BoW, TF-IDF and BM25 miss.

**Contextual Embeddings**: Contextual embeddings, generated by models such as BERT or OpenAI's text embeddings, produce high-dimensional, context-aware representations for queries and documents. These models, based on transformers, capture deep semantic meanings and relationships. For example, a query about "apple" will retrieve documents discussing apple the fruit or apple the technology company depending on the input query.

![None](https://miro.medium.com/v2/resize:fit:700/1*buw8tbSpgEo5DvhrkSyJMw.png)

Static vs Contextual Embeddings (Source: Image by Author)

**Learned Sparse Retrieval:** Generate sparse, interpretable representations using neural networks. Examples: SPLADE, DeepCT, DocT5Query

**Dense Retrieval**: Encode queries and documents as dense vectors for semantic matching. Examples: DPR (Dense Passage Retriever), ANCE, RepBERT

**Hybrid Retrieval**: Combine sparse and dense methods for balanced efficiency and effectiveness. Examples: ColBERT, COIL

**Cross-Encoder Retrieval**: Directly compare query-document pairs using transformer models. Example: BERT-based rerankers

**Graph-based Retrieval**: Leverage graph structures to model relationships between documents. Examples: TextGraphs, GraphNeural Networks for IR

**Quantum-inspired Retrieval**: Apply quantum computing principles to information retrieval. Example: Quantum Language Models (QLM)

**Neural IR models**: Encompass various neural network-based approaches to information retrieval. Examples: NPRF (Neural PRF), KNRM (Kernel-based Neural Ranking Model)

**Augmentation**: The process of combining user query and the retrieved documents from the knowledge base.

**Prompt Engineering**: The technique of giving instructions to an LLM to attain a desired outcome. The goal of Prompt Engineering is to construct the prompts to achieve accuracy and relevance in the LLM responses with respect to the desired outcome(s).

**Contextual Prompting**: Adding an instruction like "Answer only based on the context provided." to have set up the generation to focus only on the provided information and not from LLM's internal knowledge (or parametric knowledge).

**Controlled Generation Prompting**: Adding an instruction like, "If the question cannot be answered based on the provided context, say I don't know." to ensure that the model's responses are grounded in the retrieved information

**Few Shot Prompting**: LLMs adhere quite well to examples provided in the prompt. While providing the retrieved information in the prompt, specifying certain examples to help guide the generation in the way the retrieved information needs to be used.

**Chain of Thought Prompting**: It has been observed that the introduction of intermediate "reasoning" steps, improves the performance of LLMs in tasks that require complex reasoning like arithmetic, common sense, and symbolic reasoning.

**Self Consistency**: While CoT uses a single Reasoning Chain in Chain of Thought prompting, Self-Consistency aims to sample multiple diverse reasoning paths and use their respective generations to arrive at the most consistent answer

**Generated Knowledge Prompting**: This technique explores the idea of prompt-based knowledge generation by dynamically constructing relevant knowledge chains, leveraging models' latent knowledge to strengthen reasoning.

**Tree of Thoughts Prompting**: This technique maintains an explorable tree structure of coherent intermediate thought steps aimed at solving problems.

**Automatic Reasoning and Tool-use (ART)**: ART framework automatically interleaves model generations with tool use for complex reasoning tasks. ART leverages demonstrations to decompose problems and integrate tools without task-specific scripting.

**Automatic Prompt Engineer (APE)**: The APE framework automatically generates and selects optimal instructions to guide models. It leverages a large language model to synthesise candidate prompt solutions for a task based on output demonstrations.

**Active Prompt**: Active-Prompt improves Chain-of-thought methods by dynamically adapting Language Models to task-specific prompts through a process involving query, uncertainty analysis, human annotation, and enhanced inference.

**ReAct Prompting**: ReAct integrates LLMs for concurrent reasoning traces and task-specific actions, improving performance by interacting with external tools for information retrieval. When combined with CoT, it optimally utilises internal knowledge and external information, enhancing interpretability and trustworthiness of LLMs.

**Recursive Prompting**: Recursive prompting breaks down complex problems into sub-problems, solving them sequentially using prompts. This method aids compositional generalisation in tasks like math problems or question answering, with the model building on solutions from previous steps.

**Foundation Models**: Pre-trained large language models that have been trained on massive data. Some LLM developers make the model weights public for fostering collaboration and community driven innovation while others have kept the models closed offering support, managed services and better user experience

**Supervised Fine-Tuning (SFT)**: Supervised fine-tuning is a process used to adapt a pre-trained language model for specific tasks or behaviours like question-answering or chat. It involves further training a pre-trained foundation model on a labeled dataset, where the model learns to map inputs to specific desired outputs.

![None](https://miro.medium.com/v2/resize:fit:700/1*ZbNTmH66j74uCq_KonVK-w.png)

**Small Language Models (SLMs)**: Smaller models with parameter sizes in millions or a few billion offer benefits such as faster inference times, lower resource usage and easier deployment on edge devices or resource constrained environments
